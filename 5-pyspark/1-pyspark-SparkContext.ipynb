{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"count app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "print(\"Elements in RDD -> %s\" % (coll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach(f)\n",
    "\n",
    "print in terminal:\n",
    "*java\n",
    "hadoop\n",
    "spark\n",
    "akka\n",
    "spark vs hadoop\n",
    "pyspark\n",
    "pyspark and spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "fore = words.foreach(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD -> %s\" % (filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map(f, preservesPartitioning = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair -> [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair -> %s\" % (mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print(\"Adding all the elements -> %i\" % (adding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join(other, numPartitions = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('hadoop', (4, 5)), ('spark', (1, 2))]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print(\"Join RDD -> %s\" % (final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got chached > True\n"
     ]
    }
   ],
   "source": [
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(\"Words got chached > %s\" % (caching))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
